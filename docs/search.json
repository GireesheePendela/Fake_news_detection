[
  {
    "objectID": "vectorization.html",
    "href": "vectorization.html",
    "title": "Vectorization",
    "section": "",
    "text": "After cleaning and preprocessing the news articles, the next step was to convert the text data into a numerical format that machine learning models can understand.\nWe used TF-IDF Vectorization (Term Frequency–Inverse Document Frequency) to represent each article as a vector of weighted word features.\n\n\n\nTF-IDF helps: - Emphasize important words in a document that are less frequent across all documents - Reduce the impact of common words that appear in many articles - Create a sparse, high-dimensional feature space suitable for models like Logistic Regression and Naive Bayes\n\n\n\n\n\nTF (Term Frequency)\nMeasures how often a word appears in a single document.\nIDF (Inverse Document Frequency)\nMeasures how rare a word is across all documents in the dataset.\nTF × IDF = TF-IDF\nThe final score reflects both importance and uniqueness of each word per document.\n\n\n\n\n\n\nWe used TfidfVectorizer from scikit-learn\nParameters:\n\nstop_words='english' — ignores common English stopwords\nmax_df=0.7 — ignores words that appear in more than 70% of documents\n\nThe output is a sparse matrix used as input features X for model training.\n\n\nThis vectorized data formed the foundation for training our machine learning models."
  },
  {
    "objectID": "vectorization.html#text-vectorization",
    "href": "vectorization.html#text-vectorization",
    "title": "Vectorization",
    "section": "",
    "text": "After cleaning and preprocessing the news articles, the next step was to convert the text data into a numerical format that machine learning models can understand.\nWe used TF-IDF Vectorization (Term Frequency–Inverse Document Frequency) to represent each article as a vector of weighted word features.\n\n\n\nTF-IDF helps: - Emphasize important words in a document that are less frequent across all documents - Reduce the impact of common words that appear in many articles - Create a sparse, high-dimensional feature space suitable for models like Logistic Regression and Naive Bayes\n\n\n\n\n\nTF (Term Frequency)\nMeasures how often a word appears in a single document.\nIDF (Inverse Document Frequency)\nMeasures how rare a word is across all documents in the dataset.\nTF × IDF = TF-IDF\nThe final score reflects both importance and uniqueness of each word per document.\n\n\n\n\n\n\nWe used TfidfVectorizer from scikit-learn\nParameters:\n\nstop_words='english' — ignores common English stopwords\nmax_df=0.7 — ignores words that appear in more than 70% of documents\n\nThe output is a sparse matrix used as input features X for model training.\n\n\nThis vectorized data formed the foundation for training our machine learning models."
  },
  {
    "objectID": "notebooks/vectorization.html",
    "href": "notebooks/vectorization.html",
    "title": "TF-IDF Vectorization",
    "section": "",
    "text": "df = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\cleaned_news.csv\")\nprint(df.columns)  # Confirm clean_text is present\n\nIndex(['title', 'text', 'subject', 'date', 'label', 'clean_text'], dtype='object')\n\n\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Load cleaned data\ndf = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\cleaned_news.csv\")\n\n# Drop rows with missing clean_text\ndf = df.dropna(subset=['clean_text'])\n\n# Initialize TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Fit and transform the 'clean_text' column\nX = vectorizer.fit_transform(df['clean_text'])\n\n# Target variable\ny = df['label']\n\n# Train-test split (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Check dimensions\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\n\nX_train shape: (35412, 218495)\nX_test shape: (8854, 218495)\n\n\n\nprint(df['clean_text'].isna().sum())  # Count NaNs\n\n0"
  },
  {
    "objectID": "notebooks/training_model2.html",
    "href": "notebooks/training_model2.html",
    "title": "Train model using Multinomial Naive Bayes",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n#  Load the cleaned dataset\ndf = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\cleaned_news.csv\")\ndf = df.dropna(subset=['clean_text'])\n\n#  TF-IDF Vectorization\nvectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\nX = vectorizer.fit_transform(df['clean_text'])\ny = df['label']\n\n#  Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n#  Logistic Regression\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n#  Evaluation Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\" Accuracy:  {accuracy:.4f}\")\nprint(f\" Precision: {precision:.4f}\")\nprint(f\" Recall:    {recall:.4f}\")\nprint(f\" F1 Score:  {f1:.4f}\")\n\n Accuracy:  0.9845\n Precision: 0.9868\n Recall:    0.9815\n F1 Score:  0.9841\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Train the model\nnb_model = MultinomialNB()\nnb_model.fit(X_train, y_train)\n\n# Predict\nnb_pred = nb_model.predict(X_test)\n\n# Evaluate\nnb_accuracy = accuracy_score(y_test, nb_pred)\nnb_precision = precision_score(y_test, nb_pred)\nnb_recall = recall_score(y_test, nb_pred)\nnb_f1 = f1_score(y_test, nb_pred)\n\nprint(\" Multinomial Naive Bayes Evaluation:\")\nprint(f\"Accuracy:  {nb_accuracy:.4f}\")\nprint(f\"Precision: {nb_precision:.4f}\")\nprint(f\"Recall:    {nb_recall:.4f}\")\nprint(f\"F1 Score:  {nb_f1:.4f}\")\n\n Multinomial Naive Bayes Evaluation:\nAccuracy:  0.9381\nPrecision: 0.9317\nRecall:    0.9425\nF1 Score:  0.9371\n\n\nconfusion_matrix\n\nnb_cm = confusion_matrix(y_test, nb_pred)\nlabels = ['Fake', 'Real']\n\nplt.figure(figsize=(6, 5))\nax = sns.heatmap(nb_cm, annot=False, cmap='Purples', xticklabels=labels, yticklabels=labels, cbar=False)\n\nfor i in range(nb_cm.shape[0]):\n    for j in range(nb_cm.shape[1]):\n        ax.text(j + 0.5, i + 0.5, nb_cm[i, j], ha='center', va='center', color='black', fontsize=14)\n\nax.set_title(\"Confusion Matrix - Naive Bayes\", fontsize=14)\nax.set_xlabel(\"Predicted Label\", fontsize=12)\nax.set_ylabel(\"Actual Label\", fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nROC curve\n\nfrom sklearn.metrics import roc_curve, auc\n\n# Predict probabilities\ny_proba = nb_model.predict_proba(X_test)[:, 1]\n\n# ROC values\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC\nplt.figure(figsize=(6, 5))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWord Cloud\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Create separate text blobs for each class\nfake_text = \" \".join(df[df['label'] == 0]['clean_text'])\nreal_text = \" \".join(df[df['label'] == 1]['clean_text'])\n\n#  Word cloud for fake news (red tint)\nwc_fake = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(fake_text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wc_fake, interpolation='bilinear')\nplt.axis('off')\nplt.title(\" Word Cloud - Fake News\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n#  Word cloud for real news (green tint)\nwc_real = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(real_text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wc_real, interpolation='bilinear')\nplt.axis('off')\nplt.title(\" Word Cloud - Real News\", fontsize=16)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/load_and_combine_data.html",
    "href": "notebooks/load_and_combine_data.html",
    "title": "Load and Combing the Data",
    "section": "",
    "text": "import pandas as pd\n\n# Load the datasets\nfake_df = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\projects\\fake_news_detection\\data\\Fake.csv\")\nreal_df = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\projects\\fake_news_detection\\data\\True.csv\")\n\n# Add labels\nfake_df['label'] = 0  # 0 = Fake\nreal_df['label'] = 1  # 1 = Real\n\n# Combine the two datasets\ndf = pd.concat([fake_df, real_df])\n\n# Shuffle the rows\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# Quick peek\nprint(df.head())\nprint(df['label'].value_counts())\n\n                                               title  \\\n0  Justices hostile to Virginia Republicans in bl...   \n1  Trump to detour from campaign to visit Scotlan...   \n2   Two Days Ago, Trump Asked Russia To Hack Clin...   \n3  SHOCKING: Why Our Fed Government Will Grant “D...   \n4   Hillary’s Message To Former Miss Universe Cal...   \n\n                                                text       subject  \\\n0  WASHINGTON (Reuters) - Supreme Court justices ...  politicsNews   \n1  WASHINGTON (Reuters) - Facing slumping poll nu...  politicsNews   \n2  It s time the American people wake up to the d...          News   \n3  If we didn t know better, we d almost believe ...     left-news   \n4  Miss Universe 1996 Alicia Machado is now an Am...          News   \n\n              date  label  \n0  March 21, 2016       1  \n1   June 22, 2016       1  \n2    July 29, 2016      0  \n3      Apr 7, 2015      0  \n4     May 20, 2016      0  \nlabel\n0    23481\n1    21417\nName: count, dtype: int64\n\n\n\n# Save combined data to a new CSV file for later use\ndf.to_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\combined_news.csv\", index=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fake News Detection",
    "section": "",
    "text": "Project by Gireeshee Pendela"
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Fake News Detection",
    "section": "📌 Project Overview",
    "text": "📌 Project Overview\nThis project aims to detect whether a given news article is real or fake using Natural Language Processing (NLP) and supervised machine learning models.\nWe use a labeled dataset of real and fake news articles to: - Preprocess and clean the text - Convert it into numerical features using TF-IDF Vectorization - Train models such as Logistic Regression and Multinomial Naive Bayes - Evaluate performance using standard classification metrics - Visualize insights like confusion matrices, word clouds, and top predictive words\n\n🔍 Problem Statement\nFake news can spread rapidly and influence public opinion. This project focuses on building a model that can automatically distinguish between fake and real news articles based on textual content.\n\n\n\n📦 Dataset\nThe dataset consists of: - Fake.csv: News articles labeled as fake - True.csv: News articles labeled as real\nThe two datasets are combined and labeled (0 = fake, 1 = real), then cleaned and preprocessed for modeling.\n\n\n\n🛠️ Technologies Used\n\nPython (Pandas, NumPy, Scikit-learn)\nNatural Language Toolkit (NLTK)\nMatplotlib, Seaborn\nWordCloud\nQuarto (for documentation and static site)"
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Fake News Detection",
    "section": "📁 GitHub Repository",
    "text": "📁 GitHub Repository\n👉 View GitHub Repository"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Dataset",
    "section": "",
    "text": "The dataset used in this project comes from Kaggle: Fake and Real News Dataset, and consists of two separate CSV files:\n\nFake.csv — Contains news articles that are labeled as fake\nTrue.csv — Contains news articles that are labeled as real\n\nEach file includes the following columns: - title: The headline of the news article - text: The body/content of the article - subject: Topic category (e.g., politics, world news) - date: Publication date"
  },
  {
    "objectID": "data.html#combining-the-dataset",
    "href": "data.html#combining-the-dataset",
    "title": "Dataset",
    "section": "🔗 Combining the Dataset",
    "text": "🔗 Combining the Dataset\nTo train a supervised machine learning model, we need a single labeled dataset. So we:\n\nAssigned a new column called label:\n\n0 for fake articles\n1 for real articles\n\nUsed pandas.concat() to combine both datasets into one DataFrame\nShuffled the rows to randomize the ordering\nSaved the result as combined_news.csv\n\nThis ensures a balanced and properly labeled dataset for training and testing."
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Classification Models & Visual Insights",
    "section": "",
    "text": "We trained and evaluated two different models using TF-IDF vectorized news article data:\n\nLogistic Regression\nMultinomial Naive Bayes\n\nEach model was evaluated on performance metrics and supported with insightful visualizations.\n\n\n\nA linear classifier suitable for sparse high-dimensional data like TF-IDF.\n\n\n\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\n\nConfusion Matrix — Displays correct and incorrect predictions.\n\n\n\n\nLogistic Confusion Matrix\n\n\n\nCorrelation Matrix — Shows correlation between article text length and label.\n\n\n\n\nLogistic Correlation Matrix\n\n\n\nTop Keywords — Bar plots of the most predictive words for fake and real news.\n\n\n\n\nLogistic Top Keywords used in Real News\n\n\n\n\n\nLogistic Top Keywords used in Fake News\n\n\n\n\n\n\n\nA fast, probabilistic classifier designed for text data with discrete features.\n\n\n\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\n\nConfusion Matrix — Indicates classification performance.\n\n\n\n\nNaive Bayes Confusion Matrix\n\n\n\nROC Curve — Visualizes the trade-off between true positive and false positive rates.\n\n\n\n\nROC Curve\n\n\n\nWord Clouds:\n\n🔴 Fake News — Shows frequent terms in fake news articles\n\n\n\n\nFake News\n\n\n\n🟢 Real News — Highlights words common in real news articles\n\n\n\n\nReal News\n\n\n\n\nThese visualizations help interpret not just how well the models performed, but also why — based on word patterns and prediction behavior."
  },
  {
    "objectID": "model.html#model-training-visualizations",
    "href": "model.html#model-training-visualizations",
    "title": "Classification Models & Visual Insights",
    "section": "",
    "text": "We trained and evaluated two different models using TF-IDF vectorized news article data:\n\nLogistic Regression\nMultinomial Naive Bayes\n\nEach model was evaluated on performance metrics and supported with insightful visualizations.\n\n\n\nA linear classifier suitable for sparse high-dimensional data like TF-IDF.\n\n\n\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\n\nConfusion Matrix — Displays correct and incorrect predictions.\n\n\n\n\nLogistic Confusion Matrix\n\n\n\nCorrelation Matrix — Shows correlation between article text length and label.\n\n\n\n\nLogistic Correlation Matrix\n\n\n\nTop Keywords — Bar plots of the most predictive words for fake and real news.\n\n\n\n\nLogistic Top Keywords used in Real News\n\n\n\n\n\nLogistic Top Keywords used in Fake News\n\n\n\n\n\n\n\nA fast, probabilistic classifier designed for text data with discrete features.\n\n\n\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\n\nConfusion Matrix — Indicates classification performance.\n\n\n\n\nNaive Bayes Confusion Matrix\n\n\n\nROC Curve — Visualizes the trade-off between true positive and false positive rates.\n\n\n\n\nROC Curve\n\n\n\nWord Clouds:\n\n🔴 Fake News — Shows frequent terms in fake news articles\n\n\n\n\nFake News\n\n\n\n🟢 Real News — Highlights words common in real news articles\n\n\n\n\nReal News\n\n\n\n\nThese visualizations help interpret not just how well the models performed, but also why — based on word patterns and prediction behavior."
  },
  {
    "objectID": "notebooks/preprocess_text.html",
    "href": "notebooks/preprocess_text.html",
    "title": "Preprocessing the data",
    "section": "",
    "text": "import pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n# Load the combined dataset\ndf = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\combined_news.csv\")\n\n\n\n# Download necessary NLTK resources (run once)\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n# Initialize stopwords and lemmatizer\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Define text preprocessing function\ndef clean_text(text):\n    text = str(text).lower()                              # Lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)                   # Remove punctuation\n    words = nltk.word_tokenize(text)                      # Tokenize\n    words = [w for w in words if w not in stop_words]     # Remove stopwords\n    words = [lemmatizer.lemmatize(w) for w in words]      # Lemmatize\n    return ' '.join(words).strip()                        # Join and remove extra spaces\n\n# Apply preprocessing to the 'text' column\ndf['clean_text'] = df['text'].apply(clean_text)\n\n# Preview results\nprint(df[['text', 'clean_text']].head())\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n                                                text  \\\n0  WASHINGTON (Reuters) - Supreme Court justices ...   \n1  WASHINGTON (Reuters) - Facing slumping poll nu...   \n2  It s time the American people wake up to the d...   \n3  If we didn t know better, we d almost believe ...   \n4  Miss Universe 1996 Alicia Machado is now an Am...   \n\n                                          clean_text  \n0  washington reuters supreme court justice monda...  \n1  washington reuters facing slumping poll number...  \n2  time american people wake danger treasonous na...  \n3  know better almost believe federal government ...  \n4  miss universe 1996 alicia machado american cit...  \n\n\n\n# Save the cleaned DataFrame to a new CSV\ndf.to_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\cleaned_news.csv\", index=False)"
  },
  {
    "objectID": "notebooks/training_model_1.html",
    "href": "notebooks/training_model_1.html",
    "title": "Train the Model using Logistic Regresiion",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n#  Load the cleaned dataset\ndf = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\Projects\\Fake_news_detection\\data\\cleaned_news.csv\")\ndf = df.dropna(subset=['clean_text'])\n\n#  TF-IDF Vectorization\nvectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\nX = vectorizer.fit_transform(df['clean_text'])\ny = df['label']\n\n#  Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n#  Logistic Regression\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n#  Evaluation Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\" Accuracy:  {accuracy:.4f}\")\nprint(f\" Precision: {precision:.4f}\")\nprint(f\" Recall:    {recall:.4f}\")\nprint(f\" F1 Score:  {f1:.4f}\")\n\n Accuracy:  0.9845\n Precision: 0.9868\n Recall:    0.9815\n F1 Score:  0.9841\n\n\nConfusion Matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nlabels = ['Fake', 'Real']\n\n# Create heatmap\nplt.figure(figsize=(6, 5))\nax = sns.heatmap(cm, annot=False, cmap='Blues', xticklabels=labels, yticklabels=labels, cbar=False)\n\n# Add manual annotations to all cells\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        text = f\"{cm[i, j]}\"\n        ax.text(j + 0.5, i + 0.5, text, ha='center', va='center', color='black', fontsize=14)\n\n# Set titles and labels\nax.set_xlabel(\"Predicted Label\", fontsize=12)\nax.set_ylabel(\"Actual Label\", fontsize=12)\nax.set_title(\" Confusion Matrix\", fontsize=14)\nax.tick_params(axis='both', labelsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nCorrelation Matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute correlation matrix\ncorr_matrix = df[['label', 'text_length']].corr()\n\n# Create the heatmap\nplt.figure(figsize=(5, 4))\nax = sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f', cbar=True)\n\n# Manually add text to all cells\nfor i in range(corr_matrix.shape[0]):\n    for j in range(corr_matrix.shape[1]):\n        text = f\"{corr_matrix.iloc[i, j]:.2f}\"\n        ax.text(j + 0.5, i + 0.5, text, ha='center', va='center', color='white', fontsize=12)\n\n# Fix axes\nax.set_xticklabels(corr_matrix.columns, rotation=0)\nax.set_yticklabels(corr_matrix.index, rotation=0)\nplt.title(\"Correlation Matrix\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTop Keywords\n\n# Get feature importance from logistic regression\nfeature_names = vectorizer.get_feature_names_out()\ncoefs = model.coef_[0]\n\n# Top 20 positive and negative words\ntop_real = np.argsort(coefs)[-20:]\ntop_fake = np.argsort(coefs)[:20]\n\nplt.figure(figsize=(10, 6))\nplt.barh(range(20), coefs[top_real], color='green')\nplt.yticks(range(20), feature_names[top_real])\nplt.title(\"Top 20 Words Predicting Real News\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.barh(range(20), coefs[top_fake], color='red')\nplt.yticks(range(20), feature_names[top_fake])\nplt.title(\"Top 20 Words Predicting Fake News\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "preprocess.html",
    "href": "preprocess.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "To prepare the news article content for machine learning, we performed a series of text preprocessing steps on the text column. This ensures that the model learns from clean, consistent, and meaningful text.\n\n\n\nLowercasing\nConverts all characters to lowercase to avoid treating “News” and “news” as different words.\nRemoving Punctuation & Special Characters\nStrips out characters like !, ., ?, and other non-alphabetic symbols to reduce noise.\nTokenization\nSplits sentences into individual words (tokens) using nltk.word_tokenize.\nStopword Removal\nEliminates common, uninformative words such as “the”, “is”, “and”, using NLTK’s stopword list.\nLemmatization\nReduces words to their base form (e.g., “running” → “run”) using WordNetLemmatizer to normalize similar words.\nWhitespace Cleanup\nJoins cleaned tokens back into a single string with extra spaces removed.\n\n\n\n\n\nA new column called clean_text was created to store the processed text.\nThe final dataset (cleaned_news.csv) was used for model training and evaluation.\n\n\nThese preprocessing steps help remove noise, standardize input, and improve model performance by focusing on the most important words in each article."
  },
  {
    "objectID": "preprocess.html#text-cleaning-preprocessing",
    "href": "preprocess.html#text-cleaning-preprocessing",
    "title": "Data Preprocessing",
    "section": "",
    "text": "To prepare the news article content for machine learning, we performed a series of text preprocessing steps on the text column. This ensures that the model learns from clean, consistent, and meaningful text.\n\n\n\nLowercasing\nConverts all characters to lowercase to avoid treating “News” and “news” as different words.\nRemoving Punctuation & Special Characters\nStrips out characters like !, ., ?, and other non-alphabetic symbols to reduce noise.\nTokenization\nSplits sentences into individual words (tokens) using nltk.word_tokenize.\nStopword Removal\nEliminates common, uninformative words such as “the”, “is”, “and”, using NLTK’s stopword list.\nLemmatization\nReduces words to their base form (e.g., “running” → “run”) using WordNetLemmatizer to normalize similar words.\nWhitespace Cleanup\nJoins cleaned tokens back into a single string with extra spaces removed.\n\n\n\n\n\nA new column called clean_text was created to store the processed text.\nThe final dataset (cleaned_news.csv) was used for model training and evaluation.\n\n\nThese preprocessing steps help remove noise, standardize input, and improve model performance by focusing on the most important words in each article."
  }
]